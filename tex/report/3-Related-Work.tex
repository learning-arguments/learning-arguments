\label{related_work}
\chapter{Related work}
 
Here we give a concise overview over the most relevant related work. A more detailed survey can be found in \autoref{appendix-related-work}.

\section{Argumentation}

Arguments are defeasible rules or chains of such rules that explain decisions, such as predictions and categorizations. We can find examples of such explanations in our daily lives, and, in a more formal setting, in legal decisions and medical diagnoses.

Argumentation has been formalized by \cite{dungAcceptabilityArgumentsIts1995} and implemented in \textit{ASPIC+} \citep{modgilASPICFrameworkStructured2014}. \cite{roosResolvingConflictsArguments2000} formalizes argumentation in the context of logic. Arguments can be built from a \textit{defeasible theory}, consisting of facts, strict rules, and defeasible rules. The defeasible rules are partially ordered by a preference relation, such that in the case of conflicting rules, the conflict can be resolved by ignoring the less preferred rule. The preference relation can be derived by ordering the defeasible rules according to their specificity. Reasoning with defeasible theories is possible by using an argumentation tableau \citep{roosSemanticTableauMethod2020}. An overview over implementations of argumentation is provided by the \textit{Tweety Project} \cite{thimmTweetyProjectComprehensiveCollection2021}.

\section{Learning arguments}

The learning of rules or arguments can be regarded as either a supervised or unsupervised learning task. Framing the task as unsupervised puts emphasis on the quality of the arguments themselves; framing it as supervised puts emphasis on the quality of the predictions. 
% Here we are mostly concerned with the quality of the rules themselves, rather than with their predictive accuracy, so the task is best framed as unsupervised. 
The representation of both data and hypotheses is a relational representation in the original approach in \cite{verheijProofProbabilities2017}, restricted to propositional logic. In this project we investigate transfering the approach to work on input data with an attribute-value representation. (On machine learning representations, see \cite{deraedtLogicalRelationalLearning2008}.)

\cite{kakasAbductionArgumentationExplainable2020} give an insightful overview over argumentation in machine learning, enumerating multiple use cases of arguments. Here we are concerned with argumentation as the target language for learning. Within this use case, they distinguish two paradigms. In the first paradigm, arguments are potentially large monolithic rules that directly map input facts to output facts. This paradigm comprises decision lists, exception lists, inductive logic programming with exceptions, and random forest methods. In the second paradigm, arguments consist of multiple chained smaller arguments, with intermediate concepts connecting the arguments. The smaller arguments describe local relations, that is, relations that only involve a small number of attributes. Within this paradigm fall the \textit{NERD} algorithm \citep{michaelCognitiveReasoningLearning2016}, \textit{machine coaching}, and \textit{SLAP}.

Two algorithms are explicitly concerned with the mining of defeasible rules: Firstly, the \textit{DefGen} algorithm uses association rule mining, for which highly optimized algorithms for big data exist, and postprocesses the output by applying relevance criteria \citep{governatoriApplicationAssociationRules2001}. This high-level structure can also be found in our \textit{pruned search} algorithm introduced in \autoref{sec:pruned-search-meth}, but the pruned search algorithm has been developed independently from DefGen. Secondly, the \textit{HeRO} algorithm iteratively applies the criterion of \textit{information gain}, taking inspiration from decision list mining and covering rule algorithms. We have implemented the HeRO algorithm and give an overview over it in \autoref{hero-meth}.

\section{Other rule-based learning approaches}

Competing approaches for the explainable learning of rules are decision trees, relational learning and inductive logic programming, and probabilistic and causal networks.

While decision trees are equivalent to sets of classification rules (\cite{wittenDataMiningPractical2017}[ch. 3.4], \cite{hanDataMiningConcepts2011}[p. 358]), the rules to which they correspond are long and unstructured; domain experts prefer to work with well-structured sets of arguments, which then can be easily transformed into decision trees for classification \citep{breidenbachTextCode2021}. The advantage of decision trees is their suitability for big data. Some of the mentioned disadvantages can be overcome by pruning the decision tree (see also \autoref{dectrees-meth}).

Relational learning and inductive logic programming are concerned with the learning of first-order logic and logic program representations, respectively, which can potentially be downgraded to work on propositional logic or attribute-value representations \citep{deraedtLogicalRelationalLearning2008}. Usually, algorithms in these fields produce monotonic rules. These do also allow for the construction of arguments, but these arguments cannot defeat each other and are therefore less similar to everyday argumentation than arguments from nonmonotonic rules. One possibility for simulating exceptions is to use an exception predicate for each rule that has an exception. \cite{dimopoulosLearningNonmonotonicLogic1995} explore the theory of nonmonotonic logic programming, \textit{XHAIL} \citep{rayInferringProcessModels2007} and \textit{TAL} \citep{corapiInductiveLogicProgramming2010} provide algorithms.

Probabilistic networks are most suitable for reasoning with uncertainty. Causal networks present an improvement over probabilistic networks (and all other methods) by taking into account the causal relationships between the variables. Machine learning methods usually assume that future examples are drawn from the same fixed probability distribution as past examples \citep{russellArtificialIntelligenceModern2010}[p. 708, 714]. Causal networks, in contrast, allow for counterfactual reasoning \citep{russellArtificialIntelligenceModern2020}[ch. 13.5.2]; moreover, experiments indicate that it is easier to reason causally than it is to reason diagnostically \citep{kahnemanJudgmentUncertaintyHeuristics1982}[p.121-128]. Ranking theory \citep{spohnLawsBeliefRanking2012} replaces probabilities with a preference relation representing disbelief in probabilistic and causal networks, providing a potential
unexplored connection to argumentation.

\section{Propositionalization}

In this project, we aim to transfer a learning method that works on discrete propositional input data to work on attribute-value input data, including categorical and continuous attributes. Our approach here is to preprocess the input data by transforming continuous and categorical attributes into propositions. Some techniques for propositionalization are described in \cite{deraedtLogicalRelationalLearning2008}. The propositionalization techniques explored in this project are Equal-Width Binning, Equal-Depth Binning, K-Means and DBSCAN, where each of the algorithms has its respective strengths and weaknesses. Equal-Width Binning and Equal-Depth Binning are the approaches with the least complexity, and K-Means and DBSCAN are more complex.
